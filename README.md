# Masters-Thesis
@author Kevin Kraus
Masters thesis in the domain of NLP in cooperation with the IAM at UKE: Fine-tuning small LLMs for precision oncology use-cases
# MTB Patient Trial Matching based on a fine-tuned llama 2 model
## Repo structure
Beside a whole bunch of utils, scripts and data-related files the most important python files are:
- `scripts/prepare_dataset.py` for preparing the dataset for fine-tuning based on the data provided by TREC Precision Medicine Track Challenge
- `finetuning.py` for running the fine-tuning on a given `base_model` (checkpoints/meta-llama)
- `testing.py` containing of all relevant code for testing the fine-tuned model in inference (eval) mode and producing outputs that are compatible with running the official `trec_eval` evaluation script as well as the `utils/eval_utils.py` file
- `run_experiment.py` brings together all scripts to fine-tuning a model, running inference on it and evaluating it with common metrics. While doing so, all relevant parameters are tracked via mlflow. To view results of a run simply run `mflow ui` to start the tracking server.
- `main.py` Only relevant if multiple experiments should be ran sequentially. To do so, define arguments for the given keys in the `configs/experiment_definitions.yaml` (or add new keys, based on the variables in the `configs/experiments.py` file) and run `main.py`. The script will simply generate all permutations of the given inputs and create commands which are then executed.
## Setup
- Run `pip install -r requirements.txt`
- Run `pip uninstall transformer-engine`, since there is some kind of bug caused by the `peft` library import
-
### Single- vs Multi-GPU system
Pytorch has a weird bug (or feature?) that requires the definition of `CUDA_VISIBLE_DEVICES` before the `import torch` call. Hence, right now when working on a multi-gpu system it is hardcoded within the `finetuning.py` as well as the `testing.py`.
So please consider to change the value, depending in your system
- TODO: Find better solution

## Credits
This repository is an adaptaion of the llama-recipies repository by facebookresearch: https://github.com/facebookresearch/llama-recipes

# Running Experiments
## Single experiment
- run_experiment.py
- Command line arguments

## Multiple experiments
- main.py
- experiment_definitions.yaml

### Merging weights
Since we usually train with a PEFT method (LoRA) the generated output are adapter weights instead of a "full" model. Even tough they can be loaded in combination with the base-model, there is an option to merge the weights with the base-model if required (e.g. for uploading to huggingface-hub).
Run `python utils/merge_lora_weights.py --base_model BASE_MODEL_NAME --peft_model PEFT_MODEL_NAME` to merge the weights and save a new model to the same directory.


# Notes on data
- Based on .xml files of fully scraped clinical trials from clinicaltrials.gov
- Since GPU restrictions and issues in running evaluation of model (nan), max-word size was set on 1000 for now. Further experimentation will show, if more words are required, even tough the restriction only removes out ~10% of data, so still enough to work with
- First tries to use whole XML ended desastreous. No chance to fine-tune model except with e.g. 10 A100 GPUs of power.
- Right now, only simple data-cleaning, as well as max-word count are done. Further experementation is required, to see how data can be optimally prepared to achieve best results by acceptable computational costs.

## Data structure
The input data is generated by using the gold-labels of a certain year of the Precision Medicine challenge provided by the TREC Conference. Based on the gold-labels, relevant Clinical Trials are extracted from the snapshot provided. Then topics are mapped to all relevant clinical trials and the respecting label (0 = Irrelevant, 1 = Uneligible, 2 = Eligible, is added. Some basic cleaning is performed on the input dataset.
It is also worth mentioning, that due to a limit on max-tokens, only the "patient-description" XML tag is considered, since it hold the most important parts, namely "inclusion" and "exclusion" criteria


## TREC evaulation script requirements
- TOPIC_NO in the first row
- Q0 constant in the second row (0)
- NCT ID in the third row
- The RANK for the document
- The SCORE for the document (probability)
