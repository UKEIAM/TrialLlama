base_model:
#  - "Llama-2-7b-chat-hf"
#  - "Llama-2-13b-hf"
  - "Llama-2-13b-chat-hf"
dataset_version:
#  - "v1"
#  - "v2"
#  - "v3"
#  - "v4"
#  - "v5"
#  - "v5_1"
#  - "v5_2"
#  - "v6"
  - "v7"
#  - "v7_3"
#  - "v8"
#  - "v12"
dataset_test_version:
#  - "v1"
#  - "v2"
#  - "v3"
#  - "v4"
#  - "v5"
#  - "v6"
#  - "v7"
#  - "v7_3"
#  - "v8"
#  - "v8_3"
  - "v9"
#  - "v9_3"
#  - "v12"
dataset_size:
#  - 1
#  - 2
#  - 3
#  - 12
#  - 100 # POC
#  - 300
#  - 900
#  - 1800
#  - 2000
#  - 3000
#  - 5000
#  - 10000
  - None # If no dataset_size parameter is given, all available samples after topic-based balancing are taken -> 11361
num_epochs:
#  - 2
#  - 3
  - 4
#  - 5 # Tests with new scheduler etc. show optimum at 5 epochs
#  - 6
#  - 8
#  - 12
gradient_accumulation_steps:
#  - 1
#  - 2
#  - 3
#  - 4
#  - 8
#  - 12
  - 16
#  - 32
lr:
#  - 0.001
#  - 0.0001 # Default
  - 0.00002 # From original LLama 2 paper -> Fine-tuning Llama 2 to the chat version
weight_decay:
  - 0.1
#  - 0.01
temperature:
  - 0.0
#  - 0.1
#  - 0.2 # 0.2 and 0.1 show same results
#  - 0.5
#  - 0.9
#  - 1.0 # Default
#  - 1.5
#  - 2.0
#  - 3.0
#  - 5.0
#  - 10.0
evaluate_base_model:
  - false
  - true
task:
#  - classification
  - reasoning
binary_balancing:
  - true
#  - false
one_shot:
  - true
  - false
