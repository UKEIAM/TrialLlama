base_model:
  - "Llama-2-7b-chat-hf"
experiment_focus:
  - "top-p"
dataset_size:
  - 300
  - 900
  - 1800
  - 5000 # TODO: due to balancing, max. datasize currently is 17383 samples. There is more data in testing, but that would require further management to differenciate data.
  - 10000
#  - 15000
num_epochs:
  - 3
#  - 4
#  - 5
lr:
#  - 0.001
  - 0.0001
#  - 0.00001
temperature:
#  - 0.1
#  - 0.3
#  - 0.5
#  - 0.8
  - 0.9
#  - 1
top_k:
#  - 1 # Greedy strategy
#  - 10
#  - 20
#  - 30
#  - 40
#  - 50
#  - 60
#  - 70
#  - 80
top_p:
#  - 0.15
#  - 0.25
#  - 0.5
  - 0.75
  - 0.9
  - 0.95
  - 1.0 # 1.0 is kind-of deactivating top_p -> Works best based on first tests
