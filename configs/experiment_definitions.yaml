base_model:
#  - "Llama-2-7b-chat-hf"
  - "Llama-2-13b-chat-hf"
dataset_version:
#  - "v2"
  - "v3"
dataset_size:
  - 100
  - 300
  - 900
  - 1800
  - 5000
  - 10000
  - 15000 # MAX AVAILABLE DATA
num_epochs:
#  -  2
#  - 3
  - 4 # Default
#  - 5
lr:
#  - 0.001
  - 0.0001 # Default
#  - 0.00001
temperature:
#  - 0.0 # Ideally used, for deterministic output
#  - 0.1
#  - 0.2
#  - 0.95
  - 1.0 # Default
#  - 1.5
#  - 2.0
#  - 3.0
#  - 5.0
#  - 10.0
top_k:
#  - None
#  - 1 # Greedy strategy
#  - 10
#  - 20
#  - 30
#  - 40
  - 50 # Default, Experiments "llama-7b-chat-300-3-0001_top-k" show 50 seems to be best
#  - 60
#  - 70
#  - 80
#  - 90
top_p:
#  - None
#  - 0.15
#  - 0.25
#  - 0.5
#  - 0.75
#  - 0.9
#  - 0.95 # Experiments show best tradeoff between metrics and empty responses-> Works best based on first tests
#  - 0.96
#  - 0.97
#  - 0.98
  - 1.0 # Default
