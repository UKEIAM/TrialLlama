base_model:
#  - "Llama-2-7b-chat-hf"
  - "Llama-2-13b-chat-hf"
#  - "Llama-2-13b-hf"
#  - "MedLlama-2-13b-chat-hf"
dataset_version:
#  - "v1
#  - "v2"
#  - "v3"
#  - "v4"
#  - "v5"
#  - "v5_1"
#  - "v5_2"
#  - "v6"
  - "v7"
#  - "v7_3"
#  - "v8"
#  - "v12"
dataset_test_version:
#  - "v1"
#  - "v2"
#  - "v3"
#  - "v4"
#  - "v5"
#  - "v6"
  - "v7"
#  - "v7_3"
  - "v8"
#  - "v8_3"
  - "v9"
#  - "v9_3"
#  - "v12"
dataset_size:
#  - 1
#  - 2
  - 3
#  - 100 # POC
#  - 300
#  - 900
#  - 1800
#  - 3000
#  - 5000
#  - 10000
#  - 15000
#  - None # If no dataset_size parameter is given, all available samples after topic-based balancing are taken -> 15426
num_epochs:
#  -  2
#  - 3
  - 4 # Default
#  - 5
#  - 6
lr:
#  - 0.001
  - 0.0001 # Default
#  - 0.00001
temperature:
#  - 0.0 # Ideally used, for deterministic output -> Not working for now, some known issue with Llama-2. Temperature has to be higher
  - 0.1
#  - 0.2 # 0.2 and 0.1 show same results
#  - 0.5
#  - 0.9
#  - 1.0 # Default
#  - 1.5
#  - 2.0
#  - 3.0
#  - 5.0
#  - 10.0
top_k:
#  - None
#  - 1 # Greedy strategy
#  - 10
#  - 20
#  - 30
#  - 40
  - 50 # Default, Experiments "llama-7b-chat-300-3-0001_top-k" show 50 seems to be best
#  - 60
#  - 70
#  - 80
#  - 90
top_p:
#  - None
#  - 0.15
#  - 0.25
#  - 0.5
#  - 0.75
#  - 0.9
#  - 0.95
#  - 0.96
#  - 0.97
#  - 0.98
  - 1.0 # Default
evaluate_base_model:
  - false
  - true
task:
  - classification
  - reasoning
